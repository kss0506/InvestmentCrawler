"""
Main module for ETF Daily Briefing Scraper
"""
import asyncio
import logging
import sys
import os
from datetime import datetime

from config import LOG_LEVEL, LOG_FORMAT, LOG_FILE, TICKERS, TEST_TICKERS
from scheduler import ETFScraperScheduler
from scraper import ETFScraper
from telegram_sender import send_message, send_html_content, send_chart_analysis, send_briefing_as_image
from stock_data import get_stock_data

# Import Flask app for Gunicorn
from app import app

def setup_logging():
    """
    Configure logging for the application
    """
    level = getattr(logging, LOG_LEVEL.upper(), logging.INFO)
    logging.basicConfig(
        level=level,
        format=LOG_FORMAT,
        handlers=[
            logging.StreamHandler(stream=sys.stdout),
            logging.FileHandler(LOG_FILE)
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info("Logging configured successfully")
    return logger

async def run_once(tickers=None, logger=None):
    """
    Run the scraper once for all specified tickers
    
    Args:
        tickers (list, optional): List of ticker symbols. Defaults to config.TICKERS.
        logger (Logger, optional): Logger instance. If None, creates a new one.
    """
    if logger is None:
        logger = setup_logging()
        
    tickers = tickers or TICKERS
    logger.info(f"Running one-time scrape for tickers: {', '.join(tickers)}")
    
    scraper = None
    try:
        # Add overall timeout for the entire process
        scraper = ETFScraper()
        
        try:
            # Use a timeout for the entire scraping operation (2 minutes)
            results = await asyncio.wait_for(
                scraper.scrape_all_tickers(tickers),
                timeout=120
            )
            
            # Print all results
            print("\n" + "="*50)
            print(f"ETF DAILY BRIEFINGS - SINGLE RUN")
            print("="*50)
            for result in results:
                print(f"\n{result}")
                print("-"*50)
                
            # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
            try:
                logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
                today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                
                # í—¤ë” ë©”ì‹œì§€ ì „ì†¡
                header_message = f"ğŸ“Š <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ({today_date})</b>\n\n"
                await send_message(header_message)
                
                # ê° ETF/ì£¼ì‹ ë¸Œë¦¬í•‘ ì „ì†¡
                for result in results:
                    # í‹°ì»¤ ì¶”ì¶œ (ê²°ê³¼ì˜ ì²« ë¶€ë¶„ì€ í•­ìƒ "TICKER:"ë¡œ ì‹œì‘)
                    ticker = result.split(':')[0].strip()
                    
                    # HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ëœ ê²°ê³¼)
                    html_content = result.replace(f"{ticker}:", "")
                    
                    # ë¸Œë¦¬í•‘ ë‚´ìš©ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡
                    await send_briefing_as_image(ticker, html_content)
                    
                    # ì°¨íŠ¸ ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì „ì†¡
                    try:
                        chart_data = get_stock_data(ticker)
                        if chart_data:
                            await send_chart_analysis(ticker, chart_data)
                    except Exception as e:
                        logger.error(f"ì°¨íŠ¸ ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨ ({ticker}): {e}")
                    
                    # ê° í‹°ì»¤ ì‚¬ì´ì— ì•½ê°„ì˜ ì‹œê°„ ê°„ê²© ì¶”ê°€
                    await asyncio.sleep(1)
                    
                logger.info("í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
        except asyncio.TimeoutError:
            logger.error("Overall scraping operation timed out")
            # Generate fallback results for all tickers
            results = []
            for ticker in tickers:
                fallback = f"{ticker}:\në°ì¼ë¦¬ ë¸Œë¦¬í•‘\n\nì‹œê°„ ì´ˆê³¼ë¡œ ì¸í•´ ë¸Œë¦¬í•‘ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”: https://invest.zum.com/{'etf' if ticker in ['IGV', 'SOXL', 'BRKU'] else 'stock'}/{ticker}/"
                results.append(fallback)
                
            # Print fallback results
            print("\n" + "="*50)
            print(f"ETF DAILY BRIEFINGS - FALLBACK RESULTS")
            print("="*50)
            for result in results:
                print(f"\n{result}")
                print("-"*50)
                
            # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡
            try:
                logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡")
                today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                
                # ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡
                error_message = (
                    f"âš ï¸ <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ì˜¤ë¥˜ ({today_date})</b>\n\n"
                    f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¤‘ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                    f"ì¼ë¶€ ETF/ì£¼ì‹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì˜í–¥ë°›ì€ í‹°ì»¤: {', '.join(tickers)}"
                )
                await send_message(error_message)
                
                # ê° í‹°ì»¤ì— ëŒ€í•œ ëŒ€ì²´ ë©”ì‹œì§€ ì „ì†¡
                for result in results:
                    ticker = result.split(':')[0].strip()
                    await send_message(result)
                    
                logger.info("í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    except Exception as e:
        logger.error(f"Error in one-time scrape: {e}")
    finally:
        if scraper:
            scraper.close()

def run_scheduled(logger=None):
    """
    Run the scraper on a daily schedule
    
    Args:
        logger (Logger, optional): Logger instance. If None, creates a new one.
    """
    if logger is None:
        logger = setup_logging()
        
    logger.info("Starting scheduled scraper")
    scheduler = ETFScraperScheduler(tickers=TICKERS)
    scheduler.schedule_daily_run()
    scheduler.run_continuously()

if __name__ == "__main__":
    # Setup logging
    logger = setup_logging()
    logger.info("Starting ETF Daily Briefing Scraper")
    
    # Check if run mode is specified as environment variable
    run_mode = os.environ.get("RUN_MODE", "scheduled").lower()
    
    if run_mode == "single":
        # Run once for specified tickers
        asyncio.run(run_once(logger=logger))
    elif run_mode == "test":
        # Run once with test tickers (BLK only)
        logger.info("Running in test mode with limited tickers")
        asyncio.run(run_once(tickers=TEST_TICKERS, logger=logger))
    else:
        # Default to scheduled mode
        run_scheduled(logger=logger)
